# -*- coding: utf-8 -*-
"""pipeline_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ji0xWMpJkaZl3aWVZgKtn5TGEXmpxrfG
"""

# ==========================================================
# pipeline_model.py â€” for Streamlit deployment
# ==========================================================

import pandas as pd
import numpy as np
import joblib
import warnings
warnings.filterwarnings("ignore")

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score
)
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# ==========================================================
# 1. Load Data
# ==========================================================
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/train_ZoGVYWq.csv")
print(f"Data shape: {df.shape}")
display(df.head())

TARGET_COL = "renewal"

# ==========================================================
# 2. Feature Engineering
# ==========================================================
print("Applying feature engineering...")

df["age_in_years"] = (df["age_in_days"] / 365).astype(int)

df["total_late_payments"] = (
    df["Count_3-6_months_late"]
    + df["Count_6-12_months_late"]
    + df["Count_more_than_12_months_late"]
)

df["late_payment_score"] = (
    df["Count_3-6_months_late"] * 1
    + df["Count_6-12_months_late"] * 2
    + df["Count_more_than_12_months_late"] * 3
)

# ratio-based engineered features ---
df["premium_to_income"] = df["premium"] / (df["Income"] + 1)
df["late_ratio"] = df["total_late_payments"] / (df["no_of_premiums_paid"] + 1)

# Remove ID if present
if "id" in df.columns:
    df = df.drop(columns=["id"])

# ==========================================================
# 3. Split Features / Target
# ==========================================================
X = df.drop(columns=[TARGET_COL])
y = df[TARGET_COL]

# ==========================================================
# 4. Decision-tree-based Age Buckets
# ==========================================================
tree = DecisionTreeClassifier(max_leaf_nodes=5, random_state=42)
tree.fit(df[["age_in_years"]], y)
thresholds = sorted(tree.tree_.threshold[tree.tree_.threshold > 0])
bins = [df["age_in_years"].min()] + thresholds + [df["age_in_years"].max()]
labels = []
for i in range(len(bins)-1):
    if i == len(bins)-2:
        labels.append(f"{int(bins[i])}+")
    else:
        labels.append(f"{int(bins[i])}-{int(bins[i+1])}")

df["age_bucket_tree"] = pd.cut(
    df["age_in_years"],
    bins=bins,
    labels=labels,
    include_lowest=True
)


# Recompute X, y (including new bucket)
X = df.drop(columns=[TARGET_COL])
y = df[TARGET_COL]

# Identify column types
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
print(f"Numeric columns: {len(num_cols)}, Categorical columns: {len(cat_cols)}")

# ==========================================================
# 5. Preprocessing Pipelines
# ==========================================================
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, num_cols),
    ("cat", categorical_transformer, cat_cols)
])

# ==========================================================
# 6. 5-Fold Stratified Cross Validation (SMOTE inside pipeline)
# ==========================================================
print("Performing 5-Fold Stratified Cross-Validation for ROC-AUC...")

rf_cv = RandomForestClassifier(
    n_estimators=800,
    max_depth=30,
    min_samples_leaf=3,
    class_weight="balanced_subsample",
    random_state=42,
    n_jobs=-1
)

cv_pipeline = ImbPipeline(steps=[
    ("preprocessor", preprocessor),
    ("smote", SMOTE(random_state=42)),
    ("model", rf_cv)
])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
auc_scores = cross_val_score(cv_pipeline, X, y, cv=cv, scoring="roc_auc")

print(f"Mean CV ROC-AUC : {auc_scores.mean():.4f}")
print(f"Std Dev ROC-AUC : {auc_scores.std():.4f}")
print(f"All Fold Scores : {auc_scores}")

# ==========================================================
# 7. Train/Test Split
# ==========================================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# ==========================================================
# 8. Apply Preprocessing + SMOTE + Train Final Model
# ==========================================================
print("Fitting preprocessor and applying SMOTE...")
X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep = preprocessor.transform(X_test)

sm = SMOTE(random_state=42)
X_bal, y_bal = sm.fit_resample(X_train_prep, y_train)

rf_model = RandomForestClassifier(
    n_estimators=800,
    max_depth=30,
    min_samples_leaf=3,
    class_weight="balanced_subsample",
    random_state=42,
    n_jobs=-1
)

print("Training final Random Forest...")
rf_model.fit(X_bal, y_bal)

# ==========================================================
# 9. Evaluate Final Model
# ==========================================================
print("Evaluating model...")
y_pred = rf_model.predict(X_test_prep)
y_proba = rf_model.predict_proba(X_test_prep)[:, 1]

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print("\n=== Model Metrics ===")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}")
print(f"ROC-AUC  : {auc:.4f}")

# ==========================================================
#  Optional: Save Predictions with Clean Encoding
# ==========================================================
output_df = X_test.copy()
output_df["Actual"] = y_test.values
output_df["Predicted_Renewal"] = np.where(y_pred == 1, "Renew", "Not Renew")
output_df["Renewal_Probability"] = np.round(y_proba * 100, 2)

# Save with UTF-8 encoding to prevent Excel garbling issues
output_df.to_csv("predictions.csv", index=False, encoding="utf-8-sig")
print("\nPredictions saved successfully as 'predictions.csv'")


# ==========================================================
# 10. Save Preprocessor and Model
# ==========================================================
joblib.dump(preprocessor, "preprocessor.pkl")
joblib.dump(rf_model, "rf_model.pkl", compress=3)

print("\n Saved 'preprocessor.pkl' and 'rf_model.pkl' successfully!")

# ==========================================================
# 11. Feature Importance (optional)
# ==========================================================
ohe = preprocessor.named_transformers_["cat"].named_steps["encoder"]
encoded_cat_cols = list(ohe.get_feature_names_out(cat_cols))
all_feature_names = num_cols + encoded_cat_cols
importances = rf_model.feature_importances_

feat_imp = pd.DataFrame({
    "Feature": all_feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

feat_imp.to_csv("feature_importances.csv", index=False)
print("\nTop 10 Important Features:")
print(feat_imp.head(10))